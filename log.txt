19/11/2024 17:20 - A aplicação está praticamente pronta, exceto pela escassez de dados. O dicionário ainda está no início. É preciso coletar mais dados. Estou consertando alguns bugs nos scripts de treinamento.
19/11/2024 18:28 - O primeiro modelo foi treinado com gpt2 em um configuração small com sucesso (691M). {'train_runtime': 5.5762, 'train_samples_per_second': 0.897, 'train_steps_per_second': 0.897, 'train_loss': 10.890208435058593, 'epoch': 5.0}
19/11/2024 19:14 - Mudado o formato de saída do modelo de safetensors para bin. {'train_runtime': 9.0111, 'train_samples_per_second': 1.11, 'train_steps_per_second': 1.11, 'train_loss': 10.722706604003907, 'epoch': 10.0}
19/11/2024 19:48 - Teste do módulo de chat concluído com sucesso. A resposta do modelo é bem condfusa dado o tamanho do dataset.
19/11/2024 20:08 - Segunda seção, o modelo repete a entrada.
19/11/2024 20:11 - criado um novo modelo com o dataset incial. Haverá muitos resets de modelo uma vez que o dataset ainda não foi totalmente coletado
19/11/2024 20:13 - modelo criado - {'loss': 10.7218, 'grad_norm': 6.775036334991455, 'learning_rate': 1e-05, 'epoch': 10.0} {'train_runtime': 10.1532, 'train_samples_per_second': 0.985, 'train_steps_per_second': 0.985, 'train_loss': 10.72181396484375, 'epoch': 10.0} 176M  app/data/model/
19/11/2024 20:44 - O código estava sobrescrevendo o modelo anterior, então foi modificado para preservar. Reiniciando o modelo do zero.
19/11/2024 20:27 - Criado novo modelo do zero. {'loss': 10.7781, 'grad_norm': 7.358394145965576, 'learning_rate': 1e-05, 'epoch': 10.0} {'train_runtime': 9.5088, 'train_samples_per_second': 1.052, 'train_steps_per_second': 1.052, 'train_loss': 10.778144836425781, 'epoch': 10.0}
19/11/2024 20:59 - Novo modelo criado com safetensors pois o bin estava sendo sobrescrito. {'loss': 10.7079, 'grad_norm': 6.843085289001465, 'learning_rate': 1e-05, 'epoch': 10.0} {'train_runtime': 9.3356, 'train_samples_per_second': 1.071, 'train_steps_per_second': 1.071, 'train_loss': 10.707860565185547, 'epoch': 10.0}
19/11/2024 21:09 - Como o dataset incial é bem pequeno ainda, o modelo não foi capaz de gerar respostas interessantes. Na primeira interação gerou frases com palavras duplicadas e sem sentido. Na segunda interação, após o ajuste com dados do chat, o modelo repetia como um papagaio. É preciso aumentar o dataset e treinar o modelo incial novamente. Tamanho do dataset: linhas[146], tokens[1348]
19/11/2024 23:33 - Modificado o script de geração do dataset para que gere algo mais elegante. Um novo modelo foi gerado a partir desta atualização.
19/11/2024 23:39 - Colocar aspas duplas não foi uma boa ideia. O modelo responde com aspas no fim da frase. Estarei retirando aspas duplas do gerador de dataset
19/11/2024 23:44 - Modelo gerado. {'loss': 10.7007, 'grad_norm': 6.609686851501465, 'learning_rate': 1e-05, 'epoch': 10.0} {'train_runtime': 9.1417, 'train_samples_per_second': 1.094, 'train_steps_per_second': 1.094, 'train_loss': 10.70066146850586, 'epoch': 10.0}
19/11/2024 23:47 - A geração de dataset não está satisfatória. Existem duas opções: melhorar o script para que contemple todas as nunces do dicionário ou criar o dataset manualmente a medida em que é digitalizado o dicionário. Estou inclinado a escolher a segunda opção pois será uma forma de verificar a integridade dos dados. Deixar o script gerar o dataset é uma perda de oportunidade de validação humana do dataset.
21/11/2024 15:26 - Foi criado um novo dataset manualmente: linhas [73]; tokens [646]
21/11/2024 15:35 - {'loss': 10.7615, 'grad_norm': 7.200872421264648, 'learning_rate': 1e-05, 'epoch': 10.0} {'train_runtime': 9.9003, 'train_samples_per_second': 1.01, 'train_steps_per_second': 1.01, 'train_loss': 10.761488342285157, 'epoch': 10.0}
21/11/2024 15:37 - primeira interação - {user: Mamõ suípe ereîur, xe 'a? | chat: Mamõ   suípe   ereîur ,   xe   'a? -> -> sometimessometimes ->  retake ->  responds ->  ticket ->  Holdings ->  economic -> uly ->  stat -> NintendoNintendo ->  144 144 ->  candle -> diagn ->  ginger ->  predictor ->  insepar ->  screen -> $$$$ ->  HC}
21/11/2024 15:39 - treino realizado.
21/11/2024 15:39 - segunda interação - {user: Mamõ suípe ereîur, xe 'a? | chat: Mamõ   suípe   ereîur ,   xe   'a?'' , , aa , ' wik wik'ghgh ,  purely ,  Relax , killkill' worthy'xexe' -> -> ' APR'684a' midst midst ,  […]' responds responds -> a sedimenta -> xea Ria costing , 684' Rapea purely purelyaghaxe ->  midsta SAL SAL ,}
21/11/2024 15:45 - terceira interação - {user: Mamõ suípe ereîur, xe 'a? | chat: Mamõ   suípe   ereîur ,   xe   'a?''xexe' , , ' neocons'?xeaa' Brit' midst , xe sleexe SAL' sill' Piperxe Vaxe Cf' markers' Cf Cfa Cfxe , a�a Pick'jsona ,  ginger'ere'ides'suxe markersa sillatonea midstahazarda ―a SALxeerexe Pick Pickxegh' Acea mul}
21/11/2024 18:19 - Criado modelo do zero para testar conectividade. O modelo deve estar 100% offline. {'loss': 10.7743, 'grad_norm': 7.152477264404297, 'learning_rate': 1e-05, 'epoch': 10.0} {'train_runtime': 9.5219, 'train_samples_per_second': 1.05, 'train_steps_per_second': 1.05, 'train_loss': 10.774263763427735, 'epoch': 10.0}
21/11/2024 18:21 - Primeira interação: {user: Mamõ suípe ereîur, xe 'a? | chat: Mamõ   suípe   ereîur ,   xe   'a?Color zombie zombieãã Sullivan Sullivan DVD DVD faults stout stout DVD carbs carbs Handbook Cf Cf -> ->  DVD zombie tracts tractsowlerowler Simply Simply Facebook Facebookinfinf dude preseason preseason � � Cloud}
21/11/2024 18:23 - Primeiro treinamento - modelo treinado offline.